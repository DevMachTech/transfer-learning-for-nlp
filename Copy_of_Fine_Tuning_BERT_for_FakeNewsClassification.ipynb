{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Fine-Tuning BERT for FakeNewsClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevMachTech/transfer-learning-for-nlp/blob/master/Copy_of_Fine_Tuning_BERT_for_FakeNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27576fc4-2799-4edf-f924-f022e000e80b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCzb6JYtaIN7",
        "outputId": "0f177b92-4355-4236-e9dc-44f250cad314"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from datetime import date, datetime, time\n",
        "from babel.dates import format_date, format_datetime, format_time\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJrQFQgN_BE"
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/hate_speech/train.csv\", encoding='utf-8')\n",
        "df = df.iloc[:,[3,-1]]\n",
        "df.columns = [\"text\", \"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tiDhV0uWad61",
        "outputId": "72a919e7-a6ef-43df-d40e-215a3f320609"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.countplot(x=\"label\", data=df);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9807c3-c5c0-43eb-e5a3-603ebfd111aa"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20800, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676DPU1BOPdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1cb142-3c45-41f2-9cc4-c898d9a74111"
      },
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.500625\n",
              "0    0.499375\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcxQqiMsapH8"
      },
      "source": [
        "df['text']=df['text'].apply(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2MNQ6QqatYB"
      },
      "source": [
        "def clean_text(text):\n",
        "    #Remove emojis and special chars\n",
        "    clean=text\n",
        "    #reg = re.compile('\\\\.+?(?=\\B|$)')\n",
        "    #clean = text.apply(lambda r: re.sub(reg, string=r, repl=''))\n",
        "    #reg = re.compile('\\x89Û_')\n",
        "    #clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
        "    reg = re.compile('\\&amp')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='&'))\n",
        "    reg = re.compile('\\\\n')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl=' '))\n",
        "\n",
        "    #Remove hashtag symbol (#)\n",
        "    #clean = clean.apply(lambda r: r.replace('#', ''))\n",
        "\n",
        "    #Remove user names\n",
        "    reg = re.compile('@[a-zA-Z0-9\\_]+')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='@'))\n",
        "\n",
        "    #Remove URLs\n",
        "    reg = re.compile('https?\\S+(?=\\s|$)')\n",
        "    clean = clean.apply(lambda r: re.sub(reg, string=r, repl='www'))\n",
        "\n",
        "    #Lowercase\n",
        "    #clean = clean.apply(lambda r: r.lower())\n",
        "    return clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e25416a-3e6e-4f84-acbb-4a674df21488"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zOKeOMeO-DT"
      },
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAH73n39PHLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ad6ae2-57cd-4c43-ea7a-d63447f546d6"
      },
      "source": [
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "f86edf3f-9ebe-4f00-ccd9-5b2077f4beef"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2b3fc9d7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd1b8ae-7093-4060-81a8-9f826d7a5460"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      #_, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7516118e-ec46-485e-964d-d304da9e11b2"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.00123779 0.99876526]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      #elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aab0fb8-24d3-44f7-a9b1-d6b6b90bfdb6"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.354\n",
            "Validation Loss: 0.408\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.337\n",
            "Validation Loss: 0.336\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.343\n",
            "Validation Loss: 0.297\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.327\n",
            "Validation Loss: 0.296\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.328\n",
            "Validation Loss: 0.269\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.322\n",
            "Validation Loss: 0.289\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.325\n",
            "Validation Loss: 0.297\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.317\n",
            "Validation Loss: 0.267\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.318\n",
            "Validation Loss: 0.285\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    455.\n",
            "  Batch   100  of    455.\n",
            "  Batch   150  of    455.\n",
            "  Batch   200  of    455.\n",
            "  Batch   250  of    455.\n",
            "  Batch   300  of    455.\n",
            "  Batch   350  of    455.\n",
            "  Batch   400  of    455.\n",
            "  Batch   450  of    455.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     98.\n",
            "\n",
            "Training Loss: 0.312\n",
            "Validation Loss: 0.272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ec731f-b597-44f5-9ec5-b3ae7f8cdcf1"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4730b544-5696-4202-a95e-9bcbe805ceb1"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89      1558\n",
            "           1       0.91      0.86      0.88      1562\n",
            "\n",
            "    accuracy                           0.89      3120\n",
            "   macro avg       0.89      0.89      0.89      3120\n",
            "weighted avg       0.89      0.89      0.89      3120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "37c26575-110b-47bf-a3f8-96acf4994bbb"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1421</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>215</td>\n",
              "      <td>1347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1\n",
              "row_0            \n",
              "0      1421   137\n",
              "1       215  1347"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX1uTwjUPY6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}